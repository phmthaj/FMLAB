{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import copy\n",
        "import math\n",
        "import torch.nn.functional as F"
      ],
      "outputs": [],
      "metadata": {
        "id": "OaNedrhUjGb0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "outputs": [],
      "metadata": {
        "id": "I6462QxLjGb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkpWyMeGZTHy",
        "outputId": "1d95b918-3f8f-4e20-97f9-3458e81d6969"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, dim):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)"
      ],
      "outputs": [],
      "metadata": {
        "id": "DosoYt1YjGb0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, dim, max_seq_len=300):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "        pe = torch.zeros(max_seq_len, dim)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, dim, 2):\n",
        "                pe[pos, i] = math.sin(pos/ (10000 ** ((2*i)/dim)))\n",
        "                pe[pos, i+1] = math.cos(pos / (10000 ** ((2* (i+1))/dim)))\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x *math.sqrt(self.dim)\n",
        "        seq_len = x.size(1)\n",
        "        x = x + Variable(self.pe[:, :seq_len], requires_grad=False).to(device)\n",
        "        return x"
      ],
      "outputs": [],
      "metadata": {
        "id": "MJs0_6GwjGb1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "        scores = dropout(scores)\n",
        "\n",
        "    output = torch.matmul(scores, v)\n",
        "    return output"
      ],
      "outputs": [],
      "metadata": {
        "id": "wJEKoan4jGb2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.dim_head = dim // heads\n",
        "        self.h = heads\n",
        "        self.q_linear = nn.Linear(dim, dim)\n",
        "        self.k_linear = nn.Linear(dim, dim)\n",
        "        self.v_linear = nn.Linear(dim, dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        bs = q.size(0)\n",
        "\n",
        "        k = self.k_linear(k).view(bs, -1, self.h, self.dim_head)\n",
        "        q = self.q_linear(q).view(bs, -1, self.h, self.dim_head)\n",
        "        v = self.v_linear(v).view(bs, -1, self.h, self.dim_head)\n",
        "\n",
        "        k = k.transpose(1, 2)\n",
        "        q = q.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        scores = attention(q, k, v, self.dim_head, mask, self.dropout)\n",
        "\n",
        "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.dim)\n",
        "        output = self.out(concat)\n",
        "        return output"
      ],
      "outputs": [],
      "metadata": {
        "id": "4vvVcNXBjGb1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=1024, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.linear_1(x)))\n",
        "        x = self.linear_2(x)\n",
        "        return x"
      ],
      "outputs": [],
      "metadata": {
        "id": "2Cy0Xt9QjGb2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "class Norm(nn.Module):\n",
        "    def __init__(self, d_model, eps = 1e-6):\n",
        "        super().__init__()\n",
        "\n",
        "        self.size = d_model\n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "        self.eps = eps\n",
        "    def forward(self, x):\n",
        "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
        "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "        return norm"
      ],
      "outputs": [],
      "metadata": {
        "id": "v7UTrblYjGb2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.attn = MultiHeadAttention(heads, d_model)\n",
        "        self.ff = FeedForward(d_model)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.ff(x2))\n",
        "        return x\n",
        ""
      ],
      "outputs": [],
      "metadata": {
        "id": "1uYXmKKsjGb2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.norm_3 = Norm(d_model)\n",
        "\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
        "        self.attn_2 = MultiHeadAttention(heads, d_model)\n",
        "        self.ff = FeedForward(d_model).cuda()\n",
        "\n",
        "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs,\n",
        "        src_mask))\n",
        "        x2 = self.norm_3(x)\n",
        "        x = x + self.dropout_3(self.ff(x2))\n",
        "        return x\n",
        "\n",
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
      ],
      "outputs": [],
      "metadata": {
        "id": "8AWL51G_jGb3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
        "        self.norm = Norm(d_model)\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        x = self.embed(src)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
        "        x = self.embed(trg)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
        "        return self.norm(x)"
      ],
      "outputs": [],
      "metadata": {
        "id": "mQAOcVwqjGb3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads)\n",
        "        self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
        "        self.out = nn.Linear(d_model, trg_vocab)\n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        e_outputs = self.encoder(src, src_mask)\n",
        "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
        "        output = self.out(d_output)\n",
        "        return output\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "TtVVMjazjGb3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "import spacy\n",
        "import re\n",
        "class tokenize(object):\n",
        "\n",
        "    def __init__(self, lang):\n",
        "        self.nlp = spacy.load(lang)\n",
        "\n",
        "    def tokenizer(self, sentence):\n",
        "        sentence = re.sub(\n",
        "        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n",
        "        sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
        "        sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
        "        sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
        "        sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
        "        sentence = sentence.lower()\n",
        "        return [tok.text for tok in self.nlp.tokenizer(sentence) if tok.text != \" \"]"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZY3841jbjGb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets sentencepiece\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdt8I1FVZvzL",
        "outputId": "c8e6c67c-aa26-4bf7-b0ce-733bffb0d8b5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (2.18.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.3.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.12/dist-packages (from datasets) (0.7)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.3.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.22.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wmt14\", \"de-en\", split=\"train[:1%]\")  # đức ↔ anh\n",
        "print(dataset[0])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNYaOhSAZwHK",
        "outputId": "48507baf-8f3a-4c8a-b70f-1ad010f5c865"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'translation': {'de': 'Wiederaufnahme der Sitzungsperiode', 'en': 'Resumption of the session'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_data  = load_dataset(\"wmt14\", \"de-en\", split=\"test\")\n",
        "\n",
        "train_data = load_dataset(\"wmt14\", \"de-en\", split=\"train[:2%]\")\n",
        "val_data   = load_dataset(\"wmt14\", \"de-en\", split=\"validation[:2%]\")\n",
        "print(train_data[0])  # xem sample từ train\n",
        "print(val_data[0])    # sample từ validation\n",
        "print(test_data[0])   # sample từ test\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PhvNsxBZwOi",
        "outputId": "49e3af6c-11d9-487a-a870-96082e7fd422"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'translation': {'de': 'Wiederaufnahme der Sitzungsperiode', 'en': 'Resumption of the session'}}\n",
            "{'translation': {'de': 'Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'en': 'A Republican strategy to counter the re-election of Obama'}}\n",
            "{'translation': {'de': 'Gutach: Noch mehr Sicherheit für Fußgänger', 'en': 'Gutach: Increased safety for pedestrians'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q spacy\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HYYp2vHemec",
        "outputId": "873800f7-8eb1-45e1-bc08-c72a21b427c0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting de-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m138.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m124.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, OrderedDict\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "\n",
        "class tokenize(object):\n",
        "    def __init__(self, lang_model_name):\n",
        "        # Ví dụ: \"de_core_news_sm\" hoặc \"en_core_web_sm\"\n",
        "        self.nlp = spacy.load(lang_model_name)\n",
        "\n",
        "    def clean_text(self, sentence: str) -> str:\n",
        "        # Làm sạch giống code gốc của bạn\n",
        "        sentence = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n",
        "        sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
        "        sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
        "        sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
        "        sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
        "        sentence = sentence.lower().strip()\n",
        "        return sentence\n",
        "\n",
        "    def tokenizer(self, sentence: str):\n",
        "        sent = self.clean_text(sentence)\n",
        "        return [tok.text for tok in self.nlp.tokenizer(sent) if tok.text.strip() != \"\"]\n",
        "\n",
        "de_tokenizer = tokenize(\"de_core_news_sm\")\n",
        "en_tokenizer = tokenize(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "class CustomVocab:\n",
        "    def __init__(self, counter, min_freq=1, specials=('<unk>',)):\n",
        "        print(\"Đang khởi tạo vocab...\")\n",
        "\n",
        "        self.itos = list(specials)  # index -> token\n",
        "        self.stoi = OrderedDict((tok, i) for i, tok in enumerate(self.itos))\n",
        "\n",
        "        self.unk_index = self.stoi.get('<unk>', 0)\n",
        "\n",
        "        sorted_tokens = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        for token, freq in tqdm(sorted_tokens, desc=\" -> Lọc và thêm token\"):\n",
        "            if freq >= min_freq and token not in self.stoi:\n",
        "                self.stoi[token] = len(self.itos)\n",
        "                self.itos.append(token)\n",
        "\n",
        "        print(\"Khởi tạo vocab hoàn tất.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def __getitem__(self, token):\n",
        "        return self.stoi.get(token, self.unk_index)\n",
        "\n",
        "    def __call__(self, tokens):\n",
        "        return [self[token] for token in tokens]\n",
        "\n",
        "    def set_default_index(self, index):\n",
        "        self.unk_index = index\n",
        "\n",
        "def yield_tokens(data_iter, tokenizer_obj, lang_key):\n",
        "    \"\"\"\n",
        "    data_iter: iterable các item HuggingFace (mỗi item có 'translation')\n",
        "    tokenizer_obj: instance của class tokenize(...) (có .tokenizer)\n",
        "    lang_key: 'de' hoặc 'en'\n",
        "    \"\"\"\n",
        "    for item in data_iter:\n",
        "        text = item['translation'][lang_key]\n",
        "        yield tokenizer_obj.tokenizer(text)\n",
        "\n",
        "special_tokens = ['<unk>', '<pad>', '<bos>', '<eos>']  # bạn có thể giữ như vậy để dùng cho mô hình của bạn\n",
        "\n",
        "# Đức (de)\n",
        "print(\"Đang xây dựng vocab tiếng Đức (de)...\")\n",
        "print(\" -> Đang đếm tần suất (de). Quá trình này có thể mất vài phút...\")\n",
        "de_counter = Counter()\n",
        "for toks in yield_tokens(tqdm(train_data, desc=\"Đếm token DE\"), de_tokenizer, 'de'):\n",
        "    de_counter.update(toks)\n",
        "print(f\" -> Đã tìm thấy {len(de_counter)} token duy nhất (de).\")\n",
        "\n",
        "de_vocab = CustomVocab(de_counter, min_freq=2, specials=special_tokens)\n",
        "de_vocab.set_default_index(de_vocab['<unk>'])  # giữ API cũ của bạn\n",
        "\n",
        "# Anh (en)\n",
        "print(\"\\nĐang xây dựng vocab tiếng Anh (en)...\")\n",
        "print(\" -> Đang đếm tần suất (en). Quá trình này có thể mất vài phút...\")\n",
        "en_counter = Counter()\n",
        "for toks in yield_tokens(tqdm(train_data, desc=\"Đếm token EN\"), en_tokenizer, 'en'):\n",
        "    en_counter.update(toks)\n",
        "print(f\" -> Đã tìm thấy {len(en_counter)} token duy nhất (en).\")\n",
        "\n",
        "en_vocab = CustomVocab(en_counter, min_freq=2, specials=special_tokens)\n",
        "en_vocab.set_default_index(en_vocab['<unk>'])\n",
        "\n",
        "print(\"\\nHoàn tất xây dựng vocab!\")\n",
        "print(f\"Kích thước vocab 'de': {len(de_vocab)}\")\n",
        "print(f\"Kích thước vocab 'en': {len(en_vocab)}\")\n",
        "\n",
        "print(\"\\n--- Kiểm tra Vocab ---\")\n",
        "print(f\"Index của '<unk>' (de): {de_vocab['<unk>']}\")\n",
        "print(f\"Index của '<pad>' (de): {de_vocab['<pad>']}\")\n",
        "print(f\"Index của '<bos>' (de): {de_vocab['<bos>']}\")\n",
        "print(f\"Index của '<eos>' (de): {de_vocab['<eos>']}\")\n",
        "\n",
        "test_sentence = \"Das ist ein einfacher Test!\"\n",
        "test_tokens = de_tokenizer.tokenizer(test_sentence)\n",
        "print(f\"\\nCâu test (de): {test_sentence}\")\n",
        "print(f\"Tokens: {test_tokens}\")\n",
        "print(f\"Indices: {de_vocab(test_tokens)}\")\n",
        "import pickle\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dk7nbxMkdAvL",
        "outputId": "44ea2fda-4bdd-490a-cd5f-86df020a79e8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Đang xây dựng vocab tiếng Đức (de)...\n",
            " -> Đang đếm tần suất (de). Quá trình này có thể mất vài phút...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Đếm token DE: 100%|██████████| 90176/90176 [00:31<00:00, 2879.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -> Đã tìm thấy 66347 token duy nhất (de).\n",
            "Đang khởi tạo vocab...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " -> Lọc và thêm token: 100%|██████████| 66347/66347 [00:00<00:00, 2670090.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Khởi tạo vocab hoàn tất.\n",
            "\n",
            "Đang xây dựng vocab tiếng Anh (en)...\n",
            " -> Đang đếm tần suất (en). Quá trình này có thể mất vài phút...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Đếm token EN: 100%|██████████| 90176/90176 [00:17<00:00, 5154.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -> Đã tìm thấy 26378 token duy nhất (en).\n",
            "Đang khởi tạo vocab...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " -> Lọc và thêm token: 100%|██████████| 26378/26378 [00:00<00:00, 2363338.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Khởi tạo vocab hoàn tất.\n",
            "\n",
            "Hoàn tất xây dựng vocab!\n",
            "Kích thước vocab 'de': 35051\n",
            "Kích thước vocab 'en': 18137\n",
            "\n",
            "--- Kiểm tra Vocab ---\n",
            "Index của '<unk>' (de): 0\n",
            "Index của '<pad>' (de): 1\n",
            "Index của '<bos>' (de): 2\n",
            "Index của '<eos>' (de): 3\n",
            "\n",
            "Câu test (de): Das ist ein einfacher Test!\n",
            "Tokens: ['das', 'ist', 'ein', 'einfacher', 'test']\n",
            "Indices: [12, 17, 31, 4455, 6439]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "PAD_IDX = de_vocab['<pad>']\n",
        "BOS_IDX = de_vocab['<bos>']\n",
        "EOS_IDX = de_vocab['<eos>']\n",
        "\n",
        "SRC_VOCAB_SIZE = len(de_vocab)\n",
        "TRG_VOCAB_SIZE = len(en_vocab)\n",
        "D_MODEL = 256\n",
        "N_LAYERS = 6\n",
        "HEADS = 8\n",
        "DROPOUT = 0.1\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "N_EPOCHS = 5\n",
        "LR = 0.0001\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, trg_batch = [], []\n",
        "    for item in batch:\n",
        "        src_sample = item['translation']['de']\n",
        "        trg_sample = item['translation']['en']\n",
        "\n",
        "        src_tensor = torch.tensor(\n",
        "            [BOS_IDX] + de_vocab(de_tokenizer.tokenizer(src_sample)) + [EOS_IDX],\n",
        "            dtype=torch.long\n",
        "        )\n",
        "        trg_tensor = torch.tensor(\n",
        "            [BOS_IDX] + en_vocab(en_tokenizer.tokenizer(trg_sample)) + [EOS_IDX],\n",
        "            dtype=torch.long\n",
        "        )\n",
        "        src_batch.append(src_tensor)\n",
        "        trg_batch.append(trg_tensor)\n",
        "\n",
        "    src_batch_padded = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "    trg_batch_padded = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "\n",
        "    return src_batch_padded, trg_batch_padded\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "9ibd19NBntd7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(SRC_VOCAB_SIZE, TRG_VOCAB_SIZE, D_MODEL, N_LAYERS, HEADS).to(device)\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "model.apply(initialize_weights)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "metadata": {
        "id": "hJq7GIwVqL3c"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pad_mask(seq, pad_idx):\n",
        "    return (seq != pad_idx).unsqueeze(1).unsqueeze(2).to(device)\n",
        "\n",
        "def create_look_ahead_mask(seq):\n",
        "    seq_len = seq.shape[1]\n",
        "    look_ahead_mask = torch.tril(torch.ones((seq_len, seq_len), device=device)).bool()\n",
        "    return look_ahead_mask\n",
        "\n",
        "def create_masks(src, trg, pad_idx):\n",
        "    src_mask = create_pad_mask(src, pad_idx)\n",
        "    trg_pad_mask = create_pad_mask(trg, pad_idx)\n",
        "    trg_look_ahead_mask = create_look_ahead_mask(trg)\n",
        "    trg_mask = trg_pad_mask & trg_look_ahead_mask.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    return src_mask, trg_mask"
      ],
      "metadata": {
        "id": "aGwQfZ-oqZ_b"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, loader, optimizer, criterion, pad_idx):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch in tqdm(loader, desc=\"Training\"):\n",
        "        src, trg = batch\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "\n",
        "        trg_input = trg[:, :-1]\n",
        "        trg_output = trg[:, 1:]\n",
        "\n",
        "        src_mask, trg_mask = create_masks(src, trg_input, pad_idx)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg_input, src_mask, trg_mask)\n",
        "\n",
        "        output_flat = output.contiguous().view(-1, TRG_VOCAB_SIZE)\n",
        "        trg_output_flat = trg_output.contiguous().view(-1)\n",
        "\n",
        "        loss = criterion(output_flat, trg_output_flat)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(loader)"
      ],
      "metadata": {
        "id": "j9kYgvhEtMao"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_epoch(model, loader, criterion, pad_idx):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
        "            src, trg = batch\n",
        "            src = src.to(device)\n",
        "            trg = trg.to(device)\n",
        "\n",
        "            trg_input = trg[:, :-1]\n",
        "            trg_output = trg[:, 1:]\n",
        "\n",
        "            src_mask, trg_mask = create_masks(src, trg_input, pad_idx)\n",
        "\n",
        "            output = model(src, trg_input, src_mask, trg_mask)\n",
        "\n",
        "            output_flat = output.contiguous().view(-1, TRG_VOCAB_SIZE)\n",
        "            trg_output_flat = trg_output.contiguous().view(-1)\n",
        "\n",
        "            loss = criterion(output_flat, trg_output_flat)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(loader)"
      ],
      "metadata": {
        "id": "SpPtlWDdqqu0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.autograd import Variable\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train_epoch(model, train_dataloader, optimizer, criterion, PAD_IDX)\n",
        "    valid_loss = evaluate_epoch(model, val_dataloader, criterion, PAD_IDX)\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'transformer-wmt14-de-en.pt')\n",
        "\n",
        "    print(f\"\\nEpoch: {epoch+1:02} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
        "    print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}\")\n",
        "\n",
        "print(\"\\nTraining hoàn tất!\")\n",
        "print(f\"Model tốt nhất đã được lưu vào 'transformer-wmt14-de-en.pt'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scVJWa_sqtfQ",
        "outputId": "d5fe1ebc-b726-4cf2-f245-0773da4fb0f7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2818/2818 [06:26<00:00,  7.28it/s]\n",
            "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 31.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 01 | Time: 6m 27s\n",
            "\tTrain Loss: 5.207 | Train PPL: 182.505\n",
            "\t Val. Loss: 5.386 |  Val. PPL: 218.326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2818/2818 [06:21<00:00,  7.38it/s]\n",
            "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 32.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 02 | Time: 6m 22s\n",
            "\tTrain Loss: 4.354 | Train PPL:  77.816\n",
            "\t Val. Loss: 4.953 |  Val. PPL: 141.601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2818/2818 [06:23<00:00,  7.36it/s]\n",
            "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 29.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 03 | Time: 6m 23s\n",
            "\tTrain Loss: 3.914 | Train PPL:  50.084\n",
            "\t Val. Loss: 4.610 |  Val. PPL: 100.507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2818/2818 [06:21<00:00,  7.39it/s]\n",
            "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 32.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 04 | Time: 6m 21s\n",
            "\tTrain Loss: 3.568 | Train PPL:  35.435\n",
            "\t Val. Loss: 4.333 |  Val. PPL:  76.186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2818/2818 [06:23<00:00,  7.34it/s]\n",
            "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 21.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 05 | Time: 6m 24s\n",
            "\tTrain Loss: 3.298 | Train PPL:  27.053\n",
            "\t Val. Loss: 4.132 |  Val. PPL:  62.306\n",
            "\n",
            "Training hoàn tất!\n",
            "Model tốt nhất đã được lưu vào 'transformer-wmt14-de-en.pt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data  = load_dataset(\"wmt14\", \"de-en\", split=\"test\")\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from datasets import load_metric # <-- Dùng lại thư viện 'datasets' cũ\n",
        "import warnings\n",
        "\n",
        "def translate_sentence(model, sentence, de_vocab, en_vocab, de_tokenizer, device, max_len=100):\n",
        "\n",
        "    model.eval() # Đảm bảo model ở chế độ eval\n",
        "\n",
        "    # 1. Tokenize và numericalize câu nguồn (de)\n",
        "    tokens = de_tokenizer.tokenizer(sentence)\n",
        "    indices = [de_vocab['<bos>']] + de_vocab(tokens) + [de_vocab['<eos>']]\n",
        "    src_tensor = torch.LongTensor(indices).unsqueeze(0).to(device)\n",
        "\n",
        "    # 2. Tạo source mask (chỉ dùng create_pad_mask)\n",
        "    src_mask = create_pad_mask(src_tensor, de_vocab['<pad>'])\n",
        "\n",
        "    # 3. Chạy Encoder 1 LẦN DUY NHẤT\n",
        "    with torch.no_grad():\n",
        "        e_outputs = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    # 4. Vòng lặp tự hồi quy của Decoder\n",
        "    trg_indices = [en_vocab['<bos>']] # Bắt đầu câu dịch với <bos>\n",
        "\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor(trg_indices).unsqueeze(0).to(device)\n",
        "\n",
        "        # 5. Tạo target mask\n",
        "        trg_mask = create_masks(src_tensor, trg_tensor, de_vocab['<pad>'])[1]\n",
        "\n",
        "        # 6. Chạy decoder\n",
        "        with torch.no_grad():\n",
        "            output = model.decoder(trg_tensor, e_outputs, src_mask, trg_mask)\n",
        "            pred = model.out(output)\n",
        "\n",
        "        # 7. Lấy từ cuối cùng model dự đoán (Greedy search)\n",
        "        pred_token_idx = pred.argmax(2)[:, -1].item()\n",
        "\n",
        "        trg_indices.append(pred_token_idx) # Thêm từ dự đoán vào chuỗi dịch\n",
        "\n",
        "        # 8. Dừng nếu model dịch ra <eos>\n",
        "        if pred_token_idx == en_vocab['<eos>']:\n",
        "            break\n",
        "\n",
        "    # 9. Chuyển indices về lại chữ (bỏ <bos> ở đầu)\n",
        "    trg_tokens = [en_vocab.itos[i] for i in trg_indices[1:]]\n",
        "\n",
        "    # 10. Ghép lại thành câu, dừng ở <eos> nếu có\n",
        "    output_sentence = []\n",
        "    for tok in trg_tokens:\n",
        "        if tok == '<eos>':\n",
        "            break\n",
        "        output_sentence.append(tok)\n",
        "\n",
        "    return \" \".join(output_sentence)\n",
        "\n",
        "# =============================================================================\n",
        "# ▼▼▼ CODE TEST CHÍNH (ĐÃ SỬA) ▼▼▼\n",
        "# =============================================================================\n",
        "\n",
        "print(\"--- BẮT ĐẦU TEST (Dùng thư viện 'datasets' cũ) ---\")\n",
        "\n",
        "# --- Bước 1: Tải model (Nếu cần) ---\n",
        "model.eval() # LUÔN LUÔN set .eval() khi test\n",
        "\n",
        "# --- Bước 2: Tải phép đo BLEU (Dùng 'load_metric' cũ) ---\n",
        "print(\"Đang tải metric BLEU...\")\n",
        "bleu_metric = load_metric(\"bleu\", trust_remote_code=True)\n",
        "print(\"Tải metric hoàn tất.\")\n",
        "\n",
        "# --- Bước 3: Chạy dịch trên tập test ---\n",
        "print(\"Đang chạy dịch trên tập test (chỉ 2% data)...\")\n",
        "hypotheses = []  # List các *list từ* model dịch\n",
        "references = []  # List các *list của list từ* dịch mẫu\n",
        "\n",
        "# Chạy vòng lặp qua test_data\n",
        "for example in tqdm(test_data):\n",
        "    src_text = example['translation']['de']\n",
        "    trg_text = example['translation']['en']\n",
        "\n",
        "    # Dịch câu (kết quả là string)\n",
        "    prediction = translate_sentence(model, src_text, de_vocab, en_vocab, de_tokenizer, device)\n",
        "\n",
        "    # ▼▼▼ THAY ĐỔI QUAN TRỌNG Ở ĐÂY ▼▼▼\n",
        "    # Tách từ (tokenize) bằng .split() để fix lỗi ValueError\n",
        "    hypotheses.append(prediction.split())\n",
        "    references.append([trg_text.split()]) # Cả reference cũng phải .split()\n",
        "    # ▲▲▲ HẾT THAY ĐỔI ▲▲▲\n",
        "\n",
        "# --- Bước 4: Tính điểm ---\n",
        "# Bây giờ `hypotheses` và `references` đã đúng định dạng\n",
        "print(\"Đang tính điểm BLEU...\")\n",
        "final_bleu = bleu_metric.compute(predictions=hypotheses, references=references)\n",
        "\n",
        "print(\"\\n--- HOÀN TẤT TEST ---\")\n",
        "print(f\"Lưu ý: Điểm số này dựa trên tập train CHỈ CÓ 2% data.\")\n",
        "print(f\"Điểm BLEU trên tập test: {final_bleu['bleu'] * 100:.2f}\")\n",
        "\n",
        "print(\"\\n--- VÍ DỤ BẢN DỊCH ---\")\n",
        "# In ra 3 ví dụ đầu tiên (Ghép lại các từ đã split để in)\n",
        "for i in range(min(3, len(hypotheses))):\n",
        "    print(f\"\\n[{i+1}]\")\n",
        "    print(f\"  Nguồn (de): {test_data[i]['translation']['de']}\")\n",
        "    print(f\"  Mẫu (en):   {' '.join(references[i][0])}\")\n",
        "    print(f\"  Model (en): {' '.join(hypotheses[i])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOkheOfwqwn8",
        "outputId": "84aa9b6e-12f3-4754-ad80-a9256610aa70"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- BẮT ĐẦU TEST (Dùng thư viện 'datasets' cũ) ---\n",
            "Đang tải metric BLEU...\n",
            "Tải metric hoàn tất.\n",
            "Đang chạy dịch trên tập test (chỉ 2% data)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3003/3003 [14:12<00:00,  3.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Đang tính điểm BLEU...\n",
            "\n",
            "--- HOÀN TẤT TEST ---\n",
            "Lưu ý: Điểm số này dựa trên tập train CHỈ CÓ 2% data.\n",
            "Điểm BLEU trên tập test: 1.59\n",
            "\n",
            "--- VÍ DỤ BẢN DỊCH ---\n",
            "\n",
            "[1]\n",
            "  Nguồn (de): Gutach: Noch mehr Sicherheit für Fußgänger\n",
            "  Mẫu (en):   Gutach: Increased safety for pedestrians\n",
            "  Model (en): more than more than a reduction in the food safety of the food safety of the food safety\n",
            "\n",
            "[2]\n",
            "  Nguồn (de): Sie stehen keine 100 Meter voneinander entfernt: Am Dienstag ist in Gutach die neue B 33-Fußgängerampel am Dorfparkplatz in Betrieb genommen worden - in Sichtweite der älteren Rathausampel.\n",
            "  Mẫu (en):   They are not even 100 metres apart: On Tuesday, the new B 33 pedestrian lights in Dorfparkplatz in Gutach became operational - within view of the existing Town Hall traffic lights.\n",
            "  Model (en): they are no longer in a way of <unk> , on monday , the new <unk> of <unk> <unk> <unk> , <unk> <unk> <unk> in the <unk> of the elderly .\n",
            "\n",
            "[3]\n",
            "  Nguồn (de): Zwei Anlagen so nah beieinander: Absicht oder Schildbürgerstreich?\n",
            "  Mẫu (en):   Two sets of lights so close to one another: intentional or just a silly error?\n",
            "  Model (en): two of these directives are being allowed to be able to say any other or <unk> ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DtJLZS-8-EbW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}